{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FrozenLake-Mellowmax-vs-Boltzmann.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayam47/Mellowmax-RL/blob/master/FrozenLake_Mellowmax_vs_Boltzmann.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zidIrI3qYPcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "from collections import defaultdict\n",
        "import gym\n",
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtjqSSs8YV9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def interact(env, agent, num_episodes=5000, window=100):\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    best_avg_reward = -math.inf\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "    for i_episode in range(1, num_episodes+1):\n",
        "        state = env.reset() #initialize the environment\n",
        "        samp_reward = 0\n",
        "        while True:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            samp_reward += reward\n",
        "            state = next_state\n",
        "#             print(\"\\rRewards : {}\".format(samp_reward),end=\"\")\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "        \n",
        "        avg_reward=0\n",
        "        if (i_episode >= 100):\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "        averages_history.append(samp_reward)\n",
        "        if samp_reward >= 1:\n",
        "#             print('\\nEnvironment solved in {} episodes.'.format(i_episode), end=\"\")\n",
        "            return i_episode\n",
        "            break\n",
        "#         if i_episode == num_episodes: print('\\n')\n",
        "    return 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2li2MDPKYaEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.optimize import brentq\n",
        "\n",
        "class Agent:\n",
        "\n",
        "    def __init__(self, nA=4,omega=25,beta=1,mellow=0):\n",
        "        self.mellow=mellow\n",
        "        self.beta=beta\n",
        "        self.nA = nA\n",
        "        self.omega=omega\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "        self.epsilon_function = None;\n",
        "        self.next_value_function = None;\n",
        "        self.gamma = 0.9\n",
        "        self.alpha = 0.1\n",
        "           \n",
        "    def mellowmax(self,beta,state):\n",
        "        c=np.max(self.Q[state])\n",
        "        mellow=c+np.log(np.sum(np.exp(self.omega*(self.Q[state]-c)))/self.nA)/self.omega\n",
        "        ans=np.sum(np.exp((self.Q[state]-mellow)*beta)*(self.Q[state]-mellow))\n",
        "        return ans\n",
        "\n",
        "    def get_action_probability(self, state):\n",
        "        if self.mellow==1:\n",
        "            beta=brentq(self.mellowmax,-500,500,args=(state,))\n",
        "        else:\n",
        "            beta=self.beta\n",
        "        policy_s = np.exp(beta*self.Q[state])/np.sum(np.exp(beta*self.Q[state]))\n",
        "        return policy_s\n",
        "\n",
        "    def select_action(self, state):\n",
        "        policy_s = self.get_action_probability(state)\n",
        "        return np.random.choice(np.arange(self.nA), p=policy_s)\n",
        "    \n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        next_action_value = self.Q[next_state][self.select_action(next_state)]\n",
        "        self.Q[state][action] += self.alpha * (reward  + self.gamma * next_action_value - self.Q[state][action])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMm3wryCYdpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('FrozenLake-v0')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_La_u0xbYe_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZscPTNg9YhIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iteration = 50\n",
        "his_b=[] #history_of_values\n",
        "solve_his_b=[]\n",
        "for i in range(1,31):\n",
        "    b=0\n",
        "    for j in range(iteration):\n",
        "        averages_history=[]\n",
        "        agent = Agent(beta=i)\n",
        "        sol=interact(env,agent)\n",
        "        b+=sol\n",
        "    b/=iteration\n",
        "    solve_his_b.append(b)\n",
        "#         his.append(averages_history)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRoXjaRbYk9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iteration=50\n",
        "his_m=[]\n",
        "solve_his_m=[]\n",
        "for i in range(1,31):\n",
        "    b=0\n",
        "    for j in range(iteration):\n",
        "        averages_history=[]\n",
        "        agent = Agent(omega=i,mellow=1)\n",
        "        sol=interact(env,agent)\n",
        "        b+=sol\n",
        "    b/=iteration\n",
        "    solve_his_m.append(b)\n",
        "#         his.append(averages_history)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tb7Y1lZYoyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig_best=plt.figure()\n",
        "ax=fig_best.add_subplot(111)\n",
        "# for i in range(1,31,5):\n",
        "#     z=str(i)\n",
        "#     plt.plot(np.arange(0,8),his[i//5][:8],label=z)\n",
        "plt.plot(np.arange(1,31),solve_his_b,label='boltzmaan')\n",
        "plt.plot(np.arange(1,31),solve_his_m,label='mellowmax')\n",
        "plt.xlabel('omega/beta')\n",
        "plt.ylabel('number of iterations')\n",
        "plt.ylim(0,100)\n",
        "plt.legend()\n",
        "plt.savefig('lake.png')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}